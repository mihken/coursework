{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wak4DXtVYPm1",
    "outputId": "abf7950d-da95-4350-e4ab-770030b80af2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from google.colab import drive\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "# import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Flatten, Dense, Lambda\n",
    "# from keras.layers import Convolution2D\n",
    "# from keras.layers.pooling import MaxPooling2D\n",
    "# from keras.layers import Cropping2D\n",
    "# from keras.layers.core import Dense, Dropout, Activation\n",
    "# from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n",
    "# from keras import regularizers\n",
    "# from keras.layers import BatchNormalization\n",
    "# from keras.utils import np_utils\n",
    "# from keras.utils import plot_model\n",
    "import graphviz, pydot, pydotplus\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RAq_xZR0YT82"
   },
   "outputs": [],
   "source": [
    "measurements = []\n",
    "classifications = []\n",
    "\n",
    "root_folder = \"h/\"\n",
    "healthy = []\n",
    "for root, dirs, files in os.walk(root_folder):  \n",
    "    for filename in files:\n",
    "        healthy.append(filename)\n",
    "count_healthy = 0\n",
    "\n",
    "for i, name in enumerate(healthy):\n",
    "  df = pd.read_csv(\"h/\" + name, sep='\\t',skiprows=[0],\n",
    "  header=None, names=['X', 'Y', 'Wave', 'Intensity'])\n",
    "  for i in range(len(df)//1015):\n",
    "    measurements.append(df[['Intensity']][i*1015:(i+1)*1015].to_numpy())\n",
    "    classifications.append([1,0])\n",
    "    count_healthy += 1\n",
    "\n",
    "\n",
    "\n",
    "root_folder = \"s/\"\n",
    "sick = []\n",
    "for root, dirs, files in os.walk(root_folder):  \n",
    "    for filename in files:\n",
    "        sick.append(filename)\n",
    "\n",
    "count_sick = 0\n",
    "\n",
    "for i, name in enumerate(sick):\n",
    "  df = pd.read_csv(\"s/\" + name, sep='\\t',skiprows=[0],\n",
    "  header=None, names=['X', 'Y', 'Wave', 'Intensity'])\n",
    "  for i in range(len(df)//1015):\n",
    "    measurements.append(df[['Intensity']][i*1015:(i+1)*1015].to_numpy())\n",
    "    classifications.append([0,1])\n",
    "    count_sick += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qfdJ7HS1Yhch"
   },
   "outputs": [],
   "source": [
    "X = np.asarray(measurements)\n",
    "y = np.asarray(classifications)\n",
    "X = X.reshape(X.shape[0], 1015)\n",
    "y_labels = np.argmax(y, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4ttE0nANYogN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total amount of train measurements: (4761, 1015)\n",
      "Total amount of train labels: (4761, 2)\n",
      "Total amount of test measurements: (1587, 1015)\n",
      "Total amount of test labels: (1587, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=228)\n",
    "print(f'Total amount of train measurements: {X_train.shape}')\n",
    "print(f'Total amount of train labels: {y_train.shape}')\n",
    "print(f'Total amount of test measurements: {X_test.shape}')\n",
    "print(f'Total amount of test labels: {y_test.shape}')\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "72VDI9ChYtu_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# вставьте путь до модели скопировав\n",
    "model_name = 'svc_model_base.uu'\n",
    "m = pickle.load(open(model_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HR36shWwZBE3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "a = m.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "M1Vc_e-faRpf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression F1 Score 0.7139027192646495\n",
      "LogisticRegression Accuracy 0.7646502835538752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'LogisticRegression F1 Score {f1_score(y_labels, m.predict(X))}')\n",
    "print(f'LogisticRegression Accuracy {accuracy_score(y_labels, m.predict(X))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
